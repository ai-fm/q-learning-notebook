{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6os5x1FwrbuX"
      },
      "source": [
        "# Programming for Deep Reinforcement Learning\n",
        "\n",
        "Lab Course: Challenging Problems in Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9n8UrigrbuX"
      },
      "source": [
        "In this notebook, we focus on two things that can be useful to your project work and that you will encounter a lot when programming for (Deep) Reinforcement Learning (RL) -- and also general Machine Learning.\n",
        "1. Implementing a Deep RL algorithm in ```Pytorch```\n",
        "2. Using Deep RL algorithms from the ```stable_baselines3``` library\n",
        "\n",
        "Additionally, for those interested in safety in RL, we include\n",
        "3. Safe RL training using ```omnisafe``` and ```safety_gymnasium```\n",
        "\n",
        "Please fill out the code everywhere it says `# TODO: ...`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyP5vmbYrbuY"
      },
      "source": [
        "## Installing the relevant libraries\n",
        "\n",
        "Run the cell below if you don't have all necessary libraries installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd2sSl2grbuY",
        "outputId": "4bda0c49-7e3f-4e07-a9b0-a89d3e31504f"
      },
      "outputs": [],
      "source": [
        "!pip install torch -q\n",
        "!pip install gymnasium -q\n",
        "!pip install stable-baselines3 -q\n",
        "!pip install omnisafe -q\n",
        "!pip install safety-gymnasium -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJIXmNkQrbuZ"
      },
      "source": [
        "## Deep Q-Learning in Pytorch\n",
        "\n",
        "In the first notebook, we used (standard, tabular) Q-learning on two gymnasium environments. When we combine Q-learning with neural networks to represent the policy, the resulting algorithm is called DQN (Deep Q-learning). DQN is one of the most fundamental and simple algorithms in DRL. Please check the original paper [here](https://arxiv.org/pdf/1312.5602) for details on how it works!\n",
        "\n",
        "We can implement DQN (and in general, neural network based Machine Learning + RL algorithms) easily in ```pytorch```. ```pytorch``` is one of the major frameworks used for this task (there exist more e.g. Tensorflow, JAX), so understanding how to use it is a good skill to have if you like ML!\n",
        "\n",
        "In this part of the notebook, we use ```pytorch``` to implement DQN. Disclaimer: DQN is a fundamental, well-known algorithm. There are many implementations available online. Therefore, we largely use code from [this](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) pytorch tutorial here. The purpose is simply to introduce you to ML/RL coding in ```pytorch```.\n",
        "\n",
        "Additionally, we include the pseudo-code of DQN, taken from the original paper (linked above), here. Take a look and compare it to the tabular Q-learning you used last week! The major differences here lie in the use of a replay memory to store observations in the form of tuples (state, action, next state, reward), that the model is trained on, and in the use of (two) neural networks to represent the policy, instead of a Q-table.\n",
        "\n",
        "[<img src=\"DQN_algorithm.png\" width=\"750\"/>](DQN_algorithm.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "21_ju6lWrbuZ"
      },
      "outputs": [],
      "source": [
        "# First, we import the necessary libraries\n",
        "import gymnasium as gym\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LEQGQOWKrbuZ",
        "outputId": "1b404615-9d71-439b-a035-b9a517991461"
      },
      "outputs": [],
      "source": [
        "# Then, like last week, we create our gym environment.\n",
        "# TODO: create an environment for the \"CartPole-v1\" task!\n",
        "\"\"\" START COCE \"\"\"\n",
        "\n",
        "\"\"\" END CODE \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iXTqWrC4rbua"
      },
      "outputs": [],
      "source": [
        "# The replay memory (or replay buffer) stores transitions in the form of (state, action, next_state, reward)\n",
        "# In every step, the current transition is added to the replay memory.\n",
        "# During learning, batches of transition are sampled from the memory and used to update the policy\n",
        "# The replay memory is not specific to DQN, but used in many DRL algorithms.\n",
        "# We provide you with the data structure here.\n",
        "\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, next_state, reward):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(state, action, next_state, reward))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AF15NVbCrbua"
      },
      "outputs": [],
      "source": [
        "# This class is used for the neural network representation of the Q function.\n",
        "# It is based on the pytorch class nn.Module - this class is the basis of all Pytorch neural network classes.\n",
        "# It needs at least two main functions: __init__ is used to initialise the layers of the network,\n",
        "# and forward \"forwards\" the input through the layers of the network:\n",
        "# It takes the state of the RL environment as input, and predicts the next action the agent should take.\n",
        "\n",
        "# Disclaimer: We renamed this class from the Pytorch Tutorial, from DQN to QNetwork, since it does not represent the entire algorithm, only the Q function!\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, n_observations, n_actions, n_hidden):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.layer1 = nn.Linear(n_observations, n_hidden)\n",
        "        self.layer2 = nn.Linear(n_hidden, n_hidden)\n",
        "        # TODO: add the third (output) layer using the right dimensions!\n",
        "        \"\"\" START CODE \"\"\"\n",
        "\n",
        "        \"\"\" END CODE \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.layer3(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RU57coNqrbub"
      },
      "outputs": [],
      "source": [
        "# We use this function to plot the episode reward during and after training.\n",
        "episode_rewards = []\n",
        "\n",
        "def plot_rewards(show_result=False):\n",
        "    plt.figure(1)\n",
        "    if show_result:\n",
        "        plt.title('Result')\n",
        "    else:\n",
        "        plt.clf()\n",
        "        plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.plot(episode_rewards)\n",
        "\n",
        "    plt.pause(0.001)\n",
        "    if not show_result:\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "    else:\n",
        "        display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QyT6Jkumrbub"
      },
      "outputs": [],
      "source": [
        "# Now we will set some hyperparameters for training\n",
        "# Feel free to change values and watch the observe the effect they have on training!\n",
        "\n",
        "BATCH_SIZE = 128        # the number of transitions sampled from the replay buffer\n",
        "GAMMA = 0.99            # the discount factor\n",
        "EPS_START = 0.9         # the value of epsilon at the beginning of training\n",
        "EPS_END = 0.05          # the final value of epsilon at the end of training\n",
        "EPS_DECAY = 1000        # controls the rate of exponential decay of epsilon, higher means a slower decay\n",
        "TAU = 0.005             # update rate of the target network\n",
        "LR = 1e-4               # the learning rate\n",
        "N_HIDDEN = 128          # number of neurons in the hidden layer -> check out the QNetwork class!\n",
        "MEMORY_CAPACITY = 10000 # size of the replay memory\n",
        "\n",
        "# Get number of actions from gym action space\n",
        "n_actions = env.action_space.n\n",
        "# Get the number of state observations\n",
        "state, info = env.reset()\n",
        "n_observations = len(state)\n",
        "\n",
        "NUM_EPISODES = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "id": "UT7233Uzrbub",
        "outputId": "76a8a431-d8e3-4f42-fa1c-e285bffbb386"
      },
      "outputs": [],
      "source": [
        "# Now, we implement the DQN algorithm itself, following the pseudo-code from the original paper.\n",
        "\n",
        "## Algorithm 1 Deep Q-learning with Experience Replay\n",
        "\n",
        "## Initialize replay memory D to capacity N\n",
        "memory = ReplayMemory(MEMORY_CAPACITY)\n",
        "## Initialize action-value function Q with random weights\n",
        "q_network = QNetwork(n_observations, n_actions, N_HIDDEN)\n",
        "# TODO: initialize a target network with the same dimensions as the q_network!\n",
        "\"\"\" START CODE \"\"\"\n",
        "\n",
        "\"\"\" END CODE \"\"\"\n",
        "target_net.load_state_dict(q_network.state_dict())\n",
        "\n",
        "optimizer = torch.optim.AdamW(q_network.parameters(), lr=LR, amsgrad=True)\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "## for episode 1, M do\n",
        "for i_episode in range(NUM_EPISODES):\n",
        "    episode_reward = 0\n",
        "##   Initialise sequence s_1 = {x_1} and preprocessed sequenced phi_1 = phi(s_1)\n",
        "    # TODO: get the initial state from the environment using the reset() function\n",
        "    \"\"\" START CODE \"\"\"\n",
        "\n",
        "    \"\"\" END CODE \"\"\"\n",
        "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "##   for t = 1, T do\n",
        "    while True:\n",
        "##       With probability epsilon select a random action a_t\n",
        "##       otherwise select a_t = max_a Q*(phi(s_t), a; theta)\n",
        "        sample = random.random()\n",
        "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
        "        steps_done += 1\n",
        "        # TODO: implement epsilon-greedy - remember from the previous notebook, or look at the pseudo-code!\n",
        "        # For the random action, use this: torch.tensor([[env.action_space.sample()]], dtype=torch.long)\n",
        "        # For the action based on the policy, use this: q_network(state).max(1).indices.view(1,1)\n",
        "        \"\"\" START CODE \"\"\"\n",
        "\n",
        "\n",
        "        \"\"\" END CODE \"\"\"\n",
        "##       Execute action a_t in emulator and observe reward r_t and image x_{t+1}\n",
        "##       Set s_{t+1} = s_t, a_t, x_{t+1} and preprocess phi_{t+1} = phi(s_{t+1})\n",
        "\n",
        "        # TODO: using the action.item(), take a step in the environment and observe the observation, reward, terminated, truncated, _.\n",
        "        \"\"\" START CODE \"\"\"\n",
        "\n",
        "        \"\"\" END CODE \"\"\"\n",
        "        episode_reward += reward\n",
        "        reward = torch.tensor([reward])\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if terminated:\n",
        "            next_state = None\n",
        "        else:\n",
        "            next_state = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "##       Store transition (phi_t, a_t, r_t, phi_{t+1}) in D\n",
        "        # TODO: add the relevant information to the replay buffer!\n",
        "        \"\"\" START CODE \"\"\"\n",
        "\n",
        "        \"\"\" END CODE \"\"\"\n",
        "        state = next_state\n",
        "\n",
        "        if len(memory) >= BATCH_SIZE:\n",
        "##       Sample random minibatch of transitions (phi_j, a_j, r_j, phi_{j+1}) from D\n",
        "            # TODO: sample a batch of transitions of size BATCH_SIZE from the memory\n",
        "            \"\"\" START CODE \"\"\"\n",
        "\n",
        "            \"\"\" END CODE \"\"\"\n",
        "            batch = Transition(*zip(*transitions))\n",
        "##       Set y_j =   {r_j, for terminal phi_{j+1}\n",
        "##                   {r_j + gamma * max_a' Q(phi_{j+1}, a'; theta), for non-terminal phi_{j+1}\n",
        "##       Perform a gradient descent step on (y_j - Q(phi_j, a_j; theta))**2 according to equation 3\n",
        "            #optimize_model()\n",
        "\n",
        "            # Compute a mask of non-final states and concatenate the batch elements\n",
        "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                                batch.next_state)), dtype=torch.bool)\n",
        "            non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                        if s is not None])\n",
        "            state_batch = torch.cat(batch.state)\n",
        "            action_batch = torch.cat(batch.action)\n",
        "            reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "            # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken\n",
        "            # That means that for each state in the batch, we receive the action that we would have taken according to the policy\n",
        "            state_action_values = q_network(state_batch).gather(1, action_batch)\n",
        "\n",
        "            # Here, we compute the next state values accordin gto the target network\n",
        "            next_state_values = torch.zeros(BATCH_SIZE)\n",
        "            with torch.no_grad():\n",
        "                next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
        "            expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "            # Compute Huber loss\n",
        "            criterion = nn.SmoothL1Loss()\n",
        "            loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "            # Optimize the model\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_value_(q_network.parameters(), 100)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update the target network\n",
        "            target_net_state_dict = target_net.state_dict()\n",
        "            q_net_state_dict = q_network.state_dict()\n",
        "            for key in q_net_state_dict:\n",
        "                target_net_state_dict[key] = q_net_state_dict[key] * TAU + target_net_state_dict[key] * (1-TAU)\n",
        "            target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        if done:\n",
        "            plot_rewards()\n",
        "            episode_rewards.append(episode_reward)\n",
        "            break\n",
        "\n",
        "plot_rewards(show_result=True)\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fv4jMzzyKcX"
      },
      "outputs": [],
      "source": [
        "# visualize the behavious of the agent\n",
        "observation, info = env.reset()\n",
        "\n",
        "for _ in range(1000):\n",
        "    action = q_network(observation).max(1).indices.view(1,1)\n",
        "    observation, reward, terminated, truncated, _ = env.step(action.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwxlSfyArbub"
      },
      "source": [
        "## DRL with stable_baselines3\n",
        "\n",
        "Luckily, we don't have to implement all standard DRL algorithms ourselves when we want to use them. There are libraries, such as ```stable_baselines3```, that offer many different ready-to-use DRL algorithms, including DQN, which we just implemented.\n",
        "\n",
        "In the following, we check out at how to train with the ```stable_baselines3``` implementation of DQN, and then use that knowledge to train an other commonly used DRL algorithm, called PPO. Please check out the ```stable_baselines3``` [DQN documentation](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html).\n",
        "\n",
        "We will not go into the details of PPO here, but please feel free to read up on the ```stable_baselines3``` [PPO documentation](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) or take a look at the [original paper](https://arxiv.org/abs/1707.06347)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MARdfTwLrbub"
      },
      "outputs": [],
      "source": [
        "# First, we import the libraries: gymnasium for the environment, stable_baselines3 for algorithms\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import DQN, PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.logger import configure, read_csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdolJZYJrbuc",
        "outputId": "1e73dd0c-5600-49f1-961a-598d7c852508"
      },
      "outputs": [],
      "source": [
        "# Creating the environment + agent\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "dqn_model = DQN(\"MlpPolicy\", env, verbose=1)\n",
        "\n",
        "# To get an easier access to evaluation data, configure the logger to write to a file\n",
        "dqn_tmp_path = \"tmp/sb3_log/dqn\"\n",
        "new_dqn_logger = configure(dqn_tmp_path, [\"csv\"])\n",
        "dqn_model.set_logger(new_dqn_logger)\n",
        "\n",
        "# Training and saving the agent\n",
        "dqn_model.learn(total_timesteps=50000, log_interval=4)\n",
        "dqn_model.save(\"dqn_cartpole\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGDoEBNgrbuc",
        "outputId": "fca64668-b0bc-4ce4-e15e-4b16fbdaeeb2"
      },
      "outputs": [],
      "source": [
        "# Model loading and evaluation\n",
        "dqn_model = DQN.load(\"dqn_cartpole\")\n",
        "dqn_mean_rew, dqn_std_rew = evaluate_policy(dqn_model, env, n_eval_episodes=10)\n",
        "print(f\"DQN evaluation results - mean reward: {dqn_mean_rew}, std. deviation: {dqn_std_rew}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfLmijtrrbuc",
        "outputId": "1be44be6-baa7-43c5-e498-f04b78514819"
      },
      "outputs": [],
      "source": [
        "# Now, let's look at the training development, by examining the logger data.\n",
        "# read the logger output\n",
        "dqn_train_log = read_csv(\"tmp/sb3_log/progress.csv\")\n",
        "# take a quick look at the data...\n",
        "dqn_train_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAPzhkKFrbud",
        "outputId": "0ffa7681-9096-4568-dec6-a5d913cdca2d"
      },
      "outputs": [],
      "source": [
        "# Let's plot some of the training evaluation!\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# First, we plot the reward development in training\n",
        "plt.title(\"DQN mean reward during training\")\n",
        "plt.plot(dqn_train_log['time/total_timesteps'], dqn_train_log['rollout/ep_rew_mean'], label=\"dqn\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"# timesteps\")\n",
        "plt.ylabel(\"mean reward\")\n",
        "plt.show()\n",
        "\n",
        "# Let's also look at the loss\n",
        "plt.title(\"DQN training loss\")\n",
        "plt.plot(dqn_train_log['time/total_timesteps'], dqn_train_log['train/loss'], label=\"dqn\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"# timesteps\")\n",
        "plt.ylabel(\"training loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnAAdEfFrbud"
      },
      "source": [
        "Now it is your turn! Fill in the code below to train, save, load, and evaluate a PPO model on the cartpole task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sqb_Wf2Trbud",
        "outputId": "617a4680-0896-4d95-bb3d-a5b42eabda48"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\"\"\" START CODE \"\"\"\n",
        "# Create the CartPole environment\n",
        "\n",
        "\n",
        "# Create a PPO agent\n",
        "\n",
        "\n",
        "# As with DQN, we save the logger output to a file.\n",
        "\n",
        "\n",
        "\n",
        "# Train the PPO agent for 50000 timesteps (feel free to try more/less!)\n",
        "\n",
        "\n",
        "# Save the trained model to a file for later use!\n",
        "\n",
        "\"\"\" END CODE \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsocG1PKrbud",
        "outputId": "2f290b8b-1280-4c28-da5e-c94d4f5bcc57"
      },
      "outputs": [],
      "source": [
        "# Now, we want to evaluate the trained model that we saved earlier!\n",
        "\n",
        "# First, let's load the model\n",
        "ppo_model = PPO.load(\"ppo_cartpole\")\n",
        "\n",
        "# Use evaluate_policy to receive the mean and standard reward of the trained policy over e.g. 10 episodes.\n",
        "ppo_mean_rew, ppo_std_rew = evaluate_policy(ppo_model, env, n_eval_episodes=10)\n",
        "\n",
        "# Print the results\n",
        "print(f\"PPO evaluation results - mean reward: {ppo_mean_rew}, std. deviation: {ppo_std_rew}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-_pYquGrbud",
        "outputId": "97083e30-9ecf-4b38-c186-77079d27bbb3"
      },
      "outputs": [],
      "source": [
        "# TODO: Let's do some visualization again! For that, we consult the log file again.\n",
        "# read the logger output\n",
        "\"\"\" START CODE \"\"\"\n",
        "\n",
        "\"\"\" END CODE \"\"\"\n",
        "# take a quick look at the data...\n",
        "\"\"\" START CODE \"\"\"\n",
        "\n",
        "\"\"\" END CODE \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kziZ7yH1rbud",
        "outputId": "42fc06a1-0c09-4c82-b5c1-dc89834d7ca1"
      },
      "outputs": [],
      "source": [
        "# TODO: Let's plot the PPO results like before!\n",
        "# You can re-use the code from above here and add the PPO log data.\n",
        "# First, we plot the reward development in training\n",
        "\"\"\" START CODE \"\"\"\n",
        "\n",
        "\n",
        "\"\"\" END CODE \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YQTwyk3rbue"
      },
      "source": [
        "That's it!\n",
        "\n",
        "Now you saw how easy it is to train DRL models with libraries like ```stable_baselines3```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9hG2zGurbue"
      },
      "source": [
        "## Safe RL with omnisafe and safety_gymnasium\n",
        "\n",
        "An important challenge (and a very wide topic) in RL research is safety. In this part of the notebook, we look at two libraries that allow us to easily train save RL agents on suitable environments.\n",
        "\n",
        "[```safety_gymnasium```](https://safety-gymnasium.readthedocs.io/en/latest/index.html) provides environments that include notions of costs and constraints. In our task, we will use the ```SafetyCarCircle1-v0``` environment - that means a safety environment, with a Car agent, for the Circle Task, Level 1. Check out the documentation on the Circle task [here](https://safety-gymnasium.readthedocs.io/en/latest/environments/safe_navigation/circle.html#circle1)!\n",
        "\n",
        "In this Circle task, the agent learns to drive in circles around the center of a circled area. Level 0 does not implement any constraints, so it is not interesting to us here, but level 1 does! In level one, boundaries are implemented on two sides of the circle, and the agent needs to avoid crossing these boundaries. If it does, it is punished by receiving a cost.\n",
        "\n",
        "Additionally, the [```omnisafe```](https://www.omnisafe.ai/en/latest/index.html) library provides implementation of some DRL algorithms, like PPO, which we looked at in the previous section, as well as save versions of these algorithms. Here, we will use PPO, and the safe algorithm PPOLag, which implements Lagrangian constraints. Again, we do not need to understand this in detail for this notebook, but if you are interested, please read the omnisafe [documentation](https://www.omnisafe.ai/en/latest/benchmark/on-policy.html), and also consider the original papers of [PPO](https://arxiv.org/pdf/1707.06347) and the Lagrangian version [PPOLag](https://cdn.openai.com/safexp-short.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGVzGT-qrbue"
      },
      "outputs": [],
      "source": [
        "# First, we import the necessary libraries\n",
        "import omnisafe\n",
        "import safety_gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCiVlnnErbue"
      },
      "outputs": [],
      "source": [
        "# create the environment\n",
        "ENV_ID = 'SafetyCarCircle1-v0'\n",
        "env = safety_gymnasium.make(ENV_ID,\n",
        "                            render_mode='human')\n",
        "\n",
        "# We set some configurations: how many steps we want to train for, and how many steps there are in an episode\n",
        "# The number of steps per episode can be found in the Circle task documentation.\n",
        "# The documentation on which configuration settings are relevant is unfortunately a bit unclear - these two seem to be the minimum.\n",
        "# Feel free to figure out more!\n",
        "cfg = {\n",
        "    'train_cfgs': {\n",
        "        'total_steps': 100000,\n",
        "    },\n",
        "    'algo_cfgs': {\n",
        "        'steps_per_epoch': 500,\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnCvgsw7rbue"
      },
      "source": [
        "### Collapse/view training log for PPO agent\n",
        "Output log during training is very long. Collapse to not have to scroll down super far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "b8d918a24b3047febab6c8565c070238",
            "725207176eb3490882194b25ab4d0dba",
            "6e0a681f6031456db10d7fc5dde852be",
            "46f6edbfc2d44d81be66fb8489562223",
            "887b695b3a344c79b55629e8e1a8bcdc",
            "13654b36202d4cd9b2048d57ee225c96",
            "9c124371ae004c78b582533cb627c737",
            "96dd667d7f18429b83535b43d670325c",
            "a990e2cde53b431fa282a66c9d72171f",
            "8482b06fea4a43cca581f7de8626e2c1",
            "20b7b46d0a2a4034aed9c09b3b5fdc1f",
            "3cd381581f234ac080e70e2030c69646",
            "f6fe5fadc9e14758adfa993eb52166ae",
            "71313a287afc4b58bda110b2c0ca9b9d",
            "ce2d2ec86ab74e70af45fd76b7ff7a80",
            "8e807d81ee5c473f9383bccda57afa5b",
            "dc8e023a9dc94892845008e1470186ec",
            "a070d5ee6e0944098eddf1b4cf80dfef",
            "ef104fdc051f4b108f67a3a014dc9490",
            "bd813a1a085344669241d307415fbdc6",
            "468c02c6822a44a69c2a65a807cf28e8",
            "a6d3a33b2c3d4e2796cb80d2a10236ab",
            "87695ffca36642e8b7031f508e91fe98",
            "68c6f1ecb1f6435a8b4a49214838f024",
            "90870850fdc34bccb3b7873d128fd060",
            "10ecb01263a9494eab74146ca8e5f45c",
            "1e89a9cfa6ee4067a8911aacd4a97541",
            "178e8d6a9b844ef7a9cdec297fff9327",
            "a21c30ec6d6f4674bf37a872aaa48449",
            "4d4d51b7ea2c459686d13cb0f440091d",
            "faa36d9b9ee04e36adcc0fb72dd4c7c4",
            "aa6f077a00e84abd912e6e6bacc0b9bc",
            "d33be103425b4e0c8e3495d510b34f4f",
            "b5744d30658a49609dfcad8106927471",
            "bf9a57ca94924b70babffad5e935cfee",
            "6af09889e87f4cbca712e3721bd81a11",
            "9e02126462594e6c86ba984ed4c5447e",
            "e8ec186dffa94941a04593d54dbe2995",
            "a00d4f01a6134fdd853a33377a101f29",
            "ca0b919bf70445e6847e1ce1a518c249",
            "715ebe4902bb4e1bb41e06f515bd4dee",
            "f144d16163fd4c03970d9c050e0fce74",
            "3c944db1ae1b4826882da0fba002c9c0",
            "c87583be7fee4114b14fd7eef856d04c",
            "cc7e60beec2f4ca3bec4f999ad440feb",
            "217428a783944212a88c09c79ea0d140",
            "d651d5aefb3245c9b3beaa9a4d125576",
            "9cd5d2895ba64d1fb55aef3c8013ea12",
            "e5cd538292824f4f895c4942acd8f52c",
            "10c38967a31f434a8d2280986d8a9312",
            "3d18f50b730d467cabec5345f21ed887",
            "7ea2c8f5094349e8988080a852765074",
            "6b94c65217f746a78b91a884ae761a14",
            "a1ad00d2d3284606a64ee17151f9bac2",
            "08edacadc28c42edaac3c0bed6bfaa70",
            "950502cc63c24fafb48e624b73b1db2d",
            "5c8a70b68d2b4135be5b5b340437d585",
            "c2d9845dacab4010b01509b09528b6e3",
            "4efa1e9861514f3d9842ae39b5a22651",
            "24e0b6301b71436d96b5708f92739b04",
            "d88563a1cc5c491e953c3bfd0d63ed0b",
            "8456bdafa98947b0897c79de3b2e6420",
            "e2d79af6291743658c3165ae19d49d3e",
            "3a4533b06a5d4786bd9e234320b65c64",
            "c0d7a6ae02f14ba7b8f5d1d0bf18d5bb",
            "d173fa01b8f94f0cb110f8319a715f2e",
            "bfb09a94a5344033a71efcea94712753",
            "4e2786db171d4d7fa271388808a41dbb",
            "951752aa0a3a4eb4b30e13d2a2f71e58",
            "eb00ee40d6f04c0e8cb2a3cb734710aa",
            "62fb4c9aab0244d7a3a260367ee1d739",
            "2b40fc290d384d8b8df6fb0b059f8cbb",
            "1d1418ec20554906856fbf4fa185ffae",
            "8403d1c4a7d54f0989cebb28911631af",
            "5b209967b48e418dbf9b7dad841636b5",
            "6ee6377df99b4b3f9f50ccf6fa048311",
            "82a7bf09ce204870989080bbd4728526",
            "9105e679787c4a59b62748b532329631",
            "3a4f7b58ffc2494eb644955c3dfa749e",
            "9be42fdd046c4da29de739209326cf57",
            "35040071290d4f0daeee0d58711501b4",
            "f687007a4c7e4adb8871f6feef3b6c9e",
            "d4389a96b3cf4fabb834e7986d237a44",
            "17e454b81ea64490aafe780e683e8e34",
            "952bfc0015ee445db05a0f490978a8a1",
            "502bcba6e5f940e58a2d77803c3aa757",
            "ccbfab6a4c104e61b00515c4791bfcfd",
            "298ffeb75efa402d922112f71030d128",
            "a2481303442946168760dc3d0889c7ea",
            "4042dee725684b66b97f6aef44a2449d",
            "d9ec0027eda648228fd476126973f15a",
            "9e3bab02c34d49c6b35d4e7609ccd9c0",
            "1712a575ab904bf38751831a804be1be",
            "097048a71919477595cec9b44b20daa9",
            "5ed400a6574a49d6b67a49814d96e8fd",
            "08e51c08e9774190a050ea60e4b5e375",
            "3bc37b57e80c4f02ac72bb9bf748d8a8",
            "c4a462e3f295442fa0204feeecfde965",
            "acaf07c6f811425d81899270f0ba87ce",
            "1de321b6463740eba9a35970183dc5cd",
            "76caed293d8b49eab697f450e800c8f5",
            "caf1768b7ff642ac96604cb8270648b9",
            "7bac872e5bc0490ea267c9112d68f582",
            "8445c5d123dc47bca992c0a9d174c26d",
            "153373564fac43c88591352fd7c96cbe",
            "9dea797c200a495686e91daa3aca7ace",
            "269a1ea64a30497b87584546622ed7b5",
            "fa02460cc32f4e399a68d22f65fc7825",
            "d900b04b05db48b2849782859403e2c3",
            "7563d6126e0a474c8a4a5dbd5b08871c",
            "043d4408143e44b2baf2f96513b48866",
            "98064927dcba4a65a3f238aea3cabd55",
            "6c4b742de4914fe287eaea085cec0489",
            "cd80618d83f542a58ce83d233d42adfe",
            "18a7ff67f65249619990b3095cb92269",
            "c2793ece783e4a4e94e6a0fb7ab708b5",
            "1b65bac2fc5e406b851d496a45599f10",
            "b0688111cc134a38a707e25ac5398692",
            "8fd3f3dd20f648deb1ff65c213b7cc23",
            "d8bd8727a89e4d969e05b6bf096c2184",
            "2eee2d6f33d6428b9dfb7d6f534bccae",
            "b694a7065141405e8e225b51440796ad",
            "947540ba595f47c3acad55903a77dafd",
            "59fab05a2cc9450797989a9d048810fd",
            "458cda0f0e2a4bfc8cdc39599ac42f9f",
            "a464d66862f644518e2416c1120513c4",
            "297626a4a1fd49ed9d85cf22bb9fa6d5",
            "8999fded710e40c9ab69a822030c9eae",
            "5965fe351701411cb5dd26679e6f4b1c",
            "7c5567b3866f4a2182d10e8ece2e3bff",
            "588c32ab4c9341f490971a7749ebfa73",
            "f4954bdbcc674d10825a193064d10962",
            "ee83a7bd93774b539ed08064ee1ecdd8",
            "cee8252d69f24784a28ee3f9f4417908",
            "4c709dea0e8f436d81d9369ba289a3d1",
            "710ae033ab664d32a1ef6c9dc9424545",
            "da818cfc9e7b4fcab5d020ace4961575",
            "e98975e957634625967056883691b2bb",
            "b4cd7848502a411b90b4dc0abbd52e12",
            "dad9b1a4e2ed43d495ba21ee2e1556e8",
            "4c2023ea3367438c9eb05b1359f4aa24",
            "7ffdba73daa746b68a80a9135d840664",
            "be2a5955cb574eb6b9c4c212696760dc",
            "f257bd23eee44a82901289a60af65744",
            "ffd691c0f2354442972086e64d2c2fa9",
            "195db44a243a4188b7de31dd4bf7f958",
            "58f8bbacb437435d8defe30fc1e4eb05",
            "61c3f7daf1764a4780d70420c0564638",
            "1a4c30ff27e7449c8ca8709abccf3c7e",
            "95187eac849d41669eb8dbc5fbfe5ed6",
            "ade05573a05d474d849f0a4bab304739",
            "6d78146815564949ba64d9f02ec18239",
            "b25bbbbbf57646e0981296f670c49bcb",
            "fb25f5372e4e466098ddf94e6a78e18d",
            "02807826e9474b249b512a640fc415f4",
            "90a630a794b248b5ad927894015c00b6",
            "323a24a1c2d0445ab57c8cb755120440",
            "84062347160c43fe84f82248957c5cfe",
            "60cc938807ad4a5981fc75b5b7276fb1",
            "b4cb4fcccfcb41f8a56a74b732704e8e",
            "c193cc95f7564450a2e8cdac134c8f53",
            "26fdfa1897c64389a4844ecbb83da970",
            "49e026096b3541f597fdc21f44946cf8",
            "55c9e4f2089c4d1ba76a403559a16560",
            "9a690a003b0f470d9a409c7df7710409",
            "02f9fec34cac4333b80bccf7733a7f76",
            "f1f871b9253c426f989a6541f7414b4b",
            "e27106c938394f24ae573c80775a256a",
            "ef34cdaa3a1a492c8354a012aa72d97d",
            "5504db696d2b44749635d425bcdd4a85",
            "699344a98001487c8e5fc9a93ee72659",
            "cb4347976ef341fc97aeb11f5e5c993f",
            "c3a6f6502d844df3b7e603556bf6b49a",
            "962ea4d898bc4478ad644a4c77eff84d",
            "601283d648fb4376a570ae6230248d8e",
            "4af0c309975f40538a632fda1b3e20e8",
            "ea8ed28d4dde488f8bcaa9336cea8e2a",
            "ce10a12e39ec47408290a17af0881da3",
            "41b02508f5304300a7ee720a729c5831",
            "5f43749f22f54977a9f34ac49267659d",
            "738e0bee815e451081c4bdb2845ffa32",
            "d651c4189011451ea1aa962d4f4ee4df",
            "00b4e457b6b7423892556e098672dac6",
            "635e796ea5a04375b6e181c238939178",
            "ffc241aa0c6442369a88678d7811bd9a",
            "637fcf5ca9434cbdb0036e161462ee5b",
            "4df9240bdd244e41ba175f46fd6fb64b",
            "29087ff0727349629542821ad7177ef5",
            "c08ed2b1a83246bea4f611336fcf3eea",
            "1daede40bd0349628c660fa91b5c13bb",
            "067db31fa54741cd81fe159590d8dbce",
            "f98313c13f784bfa83b5dc8b8653581b",
            "848834130ff340ef8d5c1a4ef7d401d2",
            "105b0e150cae46fab303d8ec87f66b1f",
            "c4e9b60e49e147729d46dcb5260491fc",
            "792b43a279674175bb768b44ac171851",
            "4cd7679c095c408ebc1147a4dec59c08",
            "df8d3428ad914946a9dc27ec77f3cdb2",
            "fa18bb55b8104a82a10a5b3d72a100ff",
            "836cf573b08f42ae9a189c05cfb0143e",
            "d3f4b039a1fb410c9f2ff617758e694f",
            "7e08f054f0a44faa83c0fb1ef1f5bd26",
            "e0b7242b8e694f00a430e295525dc2f0",
            "ec21fe704bf34ba4b5f57b7a6799e865",
            "78932c208bb04f0fb4a39a75a511f410",
            "446b705f01d44d84bbe3350894939dda",
            "adea85e8a7a2431593da81c0bded1071",
            "defda4ab55c74148bf1905d8d2e28712",
            "adb3a994bc8a4be99890ef9dfca7802d",
            "d2a681b11fdf48daa906835bbbdb113f",
            "1b251d118e2b4608a90074a5541edc11",
            "faed45a6fa124d90815156f7ad9e1019",
            "37bba82a8caf4748941ce712490b7e39",
            "01b56ca1d5d241a59031d32c0abe6a39",
            "7186ff8df33e46839282b1e7a89e1175",
            "919081fa0b6a41788943ef201b89b78f",
            "b4c6123a03ce4bfda8043e1370b73845",
            "29896b7d52de41b3b5fe994d93cc9f71",
            "4c9d30c401d747e4904da5d26cbef422",
            "c46bf53369e44fd79a2437453afd40e9",
            "97866083653841859e58a64bc52fcea7",
            "af252a0cedcb4797bd5487408624cf12",
            "042f7e2214c645dab5e22b88daf6d6c4",
            "a8f846c0f7fe46189cf8291db18ee298",
            "0e0f2cec5a094d9ab8fdf2a01b8e74d4",
            "c15f2287aa714717b7dd367c5ea77d4f",
            "4b8a1029db604d258a7eff798ad645f9",
            "7a761fac4f044887a0cb806dbfe8ec98",
            "75d24b2233414e7ab34022198afe79fe",
            "07eb22635a20490b8fff0d525017abc7",
            "6aa46a756c6945daa77901faa040575d",
            "6d78ab3077ab416d9e4bba2fd43298c2",
            "d43a3196518e429b9906531a1d80dffc",
            "fc1555e96de4426e89a86edab00f25c7",
            "e15c783584d24174bf9419a083900b9e",
            "7d872ccc32e349a7bbf853e3079581b2",
            "c58e2722007145728635453b4503ecc2",
            "ea531ca290614ade835aa29a52964c61",
            "e4e7512371c9459da2ab7ede6a93ad03",
            "a49279a52a444e4989ed78d8f0d23b90",
            "26309ea70e154c54b1ea0041491f1678",
            "c2b55b13b2254e0db5c6249da6c047c6",
            "99c2e5dcbaa946319aa4a30f9699ecc1",
            "8eecc8c64135448eb2cbdae7d322f6e5",
            "59e287b7bf2d47d485732b1184219649",
            "8b66e61e56834c24ba66a80b2fa2f1b7",
            "3d6e3ebc9dad441e8fd3876c0ce2364d",
            "1b468c2a34af4369b2ec7407763357d4",
            "508894d10a14428ca443e595b6142bb5",
            "6999a89d8e2c42418624fe4759601c4a",
            "d1d9df19968242c1a13c5e92aae37f75",
            "a39439182bc047608139d10da9aaf5cd",
            "820abfc909064e2aa86f0ad657a69782",
            "7e8be806e0554cedb6d87e3a53ef2472",
            "31389398678b4a1e806cbb61aa4c4d2f",
            "2c5e978a61014f5e9a7c9c30a70aad54",
            "aa9ae87eddb541959bd4eeb9525a757a",
            "84282f63881247f39c79204169df6066",
            "73396affd7e140ddb0383c8d879137da",
            "d982e6e0619643baa6c283e3d64668e8",
            "0cceb6ec8eab44b297e7dd983e85dd3d",
            "ff622f5962ac487693262fc5f4d65c90",
            "8967274a78ad45cabdd1ef199e279df9",
            "6946127cc6dd405391ff490e5b3aa779",
            "fad3ecbdb2de44c491c951fd55faac67",
            "65363b35172a4cdb97b22f93ac3541b7",
            "ba4aa633a23c4cd4bda27731ce0c7bfe",
            "c6568cb25c0642f082dae91fe178914a",
            "07cd880d657643ed80ef08b961eb6a9a",
            "4a2fa2978ca94a1db90b52869e1a6a27",
            "e4d2da24767d450ab91cb9609609a4c4",
            "900cc558f26446c4b85723e2f7d09e78",
            "11299a45d26342d09a0f4652bca37300",
            "5a5dafa28d764d158db758c3fdaa8cbf",
            "580ba1c849a4464c8aea6f020ed828e2",
            "b54d66154f3c4802a84285b21e8909f4",
            "bc65f902bf934823aa3a39e963e3bccb",
            "52170d1fe6ab46adb72af7d2a5c92bbe",
            "729fcac8517248c083aa55511b9b19a4",
            "269e484b0845459f9fca49d3985a732e",
            "097931332b5f443ca6108a22bf98cd88",
            "68fb562ff04249fda795c5dbb6152ddc",
            "add103a40b0143229aa1047a018e829a",
            "53b85443db47498d9e32af899ac67f1b",
            "de893fcefe364867bf8dcfbe7724684b",
            "639a1a854e5d4ca1af77ed3a6b10e61d",
            "51167cac29b644c1aa1e832f6805dffe",
            "6e81279da414490f997067004741381a",
            "2987740d92134c6da8e4beb300ff8758",
            "2cfb2e183d4f4477aa2637ef7ba3e0e0",
            "c820882e41c24dfc92ccfaa15837c9d6",
            "78a93d42589d4fce904ec2dccb10fd5b",
            "eef28f0159d24f1bb6513844b48c8f41",
            "a2dce734139b44f28f62b474ce45d998",
            "0c3b48e5cd444b758b83052923e91656",
            "51f38b693a7749228edb703d9b130615",
            "66d86f9409214168bba89932da2e8929",
            "0e5136e0edfd4d62b257cbd1c258627b",
            "c9a5ae4983f14d019d11722f9063372c",
            "0d6ed3f40b0749b2b336db7646ab1826",
            "002bc16c52774571a2c6b049d257f6f3",
            "5e6724625f9f4e45b25b11e90cee97fc",
            "e9e47a98ccf74448abe0317a37d8970f",
            "84b475edbd5b424a9f2807d9f747b25a",
            "6225cb1c48a44e14bc097165a22b3356",
            "3cd3c82c4e634b67a295ca6f816bb647",
            "e72c1c29e1174343bae3894d891525ff",
            "e15a2aa435004975baa2988e65afe57a",
            "33fae909fd76401eab6aecac90f3ec15",
            "25c0cd7d296c4e5791af281317a88ab5",
            "ce841d75cba245f4b8ad9c2289a7a0c4",
            "7b2f66f2172f412d966a96b5cd765c12",
            "c6ee35dc438f464489bc6a2664b9388a",
            "c9885376b863471b92957bf68190373a",
            "8f4d0c96b1c6430bafafe4ee1a4b06b5",
            "9c7a05640ac34ff487206ec0c136bb91",
            "f8a61f95f56e46df9252c7e9321fb640",
            "11cd67b8e8044bbbb9d74aee1da5bebc",
            "5dcf5e72c3d244b9b2e7ff43006bb67f",
            "b798875f5d344e08a97d52f1847a4dd0",
            "2fcaab98d13846fd88930629f67c60dd",
            "d43ba510b33747cbacdedb3b4d6df3a9",
            "3e63cc18ef164a30b23d4c8fe8e51a37",
            "3579ea82dbcd4899bc06d25b4dfba9c7",
            "e0879298af6345538151f6bc6ac9e8f6",
            "2f7603a1e26b4727b53e5581ae09c5a6",
            "35d9a04a955240d2b354b5dcc6ef0256",
            "117e69207ea14474be8741039d1cd6bf",
            "11e7ba4db4444f81a88b0e86f128ac5e",
            "9fe5326e850049338afc5d1f45231767",
            "55db7c7165744a85b5016b4aec534ca6",
            "156a30a67af649f4884ebc91f33ecc61",
            "da693e0cc8664e79a7a42fc0ee222738",
            "a9f0e01641574f8492f75c7973d97075",
            "aa9ed5a57e8341b9b7d63586a8f837e1",
            "9227d3c342c34c058a0c195de496433f",
            "32f51b9e24554dff8c3693a64b583b45",
            "939414f7e75e4ee1a97ce638df1a9dba",
            "42efe8e4edb545e4ac7def99d990bda9",
            "c61b83eefd4343c4b5640a87a5191de6",
            "cd0380e3a8c54c838b040e74b7d999d7",
            "a4bdd89c29064025939a3ce96bee6907",
            "142f181ad26045f7b3d8a24d3c62401c",
            "17668b7f02bb4ca69c9d1caef72955d1",
            "353201f6f6db421ea7d9acb299d54df0",
            "f2151cf68df74584951665d5c07e65c0",
            "cea97a109d35469788976820d501dc64",
            "759a48be24d748968178a686d7536afa",
            "35cc5a6dcb8a4a288f269f5303c3f7a4",
            "1b2d091a3bb2438aa91b31e621f0b498",
            "1eff8db811294cb29b4418b9fd4be66c",
            "a76230f137504d838b82c796311c80eb",
            "f2be16165cef42a497c3cf5fff8feb29",
            "3105b68f7f8049499329fcb79ac1648a",
            "bdba2d6ea42045dab1d3a02a1494db80",
            "2f3e6c2c5e6c4fc08f166b1dea8235f8",
            "65fcb6a838514a3cac7cf2dd8675e68d",
            "a58e6dfac67840d788b9ab57eacabeca",
            "847bd1fb46fa4b1e974fa2ef11b56e35",
            "065db9ddc11d4fc08c8397bfa1168af4",
            "eac83c52ad6243b8a4df4ddf735166bd",
            "879c152e8f67447380f5510a1a883893",
            "f98d94a3affa41068f4acd8074c9a310",
            "21ada2bc7bb548e1867960828c7f0b7e",
            "6b1ba7a5bec141c39b920ce0e1ecc9d4",
            "6e0ca0d814b34a3892489b1be26879d1",
            "9f3ef470b8c9409ba35140c6b22428ce",
            "775d210798f248c89af4f79e04e53cf1",
            "7622d77a68cf4519b045e7b98a5ed1f6",
            "0289c73dfa9f4661ac6cc7b0812774f1",
            "e8b6d88c7f14430cacd25a5001d49074",
            "3f5c5780fe934e788fb73140cecc6019",
            "04bfce277b604fc787665e1ebcc649da",
            "2ab0e800864f44ef855f95bf983e2509",
            "c4c5eba93dbf453ab545d1a0a32f2463",
            "057cc10868c54768adc95276cea91161",
            "e86443d827544061b0b540c1057643ab",
            "bdf2aa708bbc42ccba983e4db822e4a3",
            "93fc6f7963d049d3ba0a3dc7e4de0f15",
            "7463d4b79db4491cb7f44ac9307b0894",
            "f48eb10e5d2b463d9455fb71c30c69aa",
            "943346b9f09748529a16eeb77fe13abf",
            "11868044a52a4d0f8a4774eb63ba506e",
            "c9035b91ca4343c7b24fddbf8e347c84",
            "9c42d8ed7f1c4a38862f9491f25983f1",
            "e6d4a8e9e9364662803af16d02f4425c",
            "a1418b886fa3438eab46d477e40b0d6e",
            "d249eabd7fc945a3b7d388fb55eb8237",
            "60a12b787a124c1db7d860bd6c344755",
            "5aee611d12254ebca2f02b528b97a9d7",
            "ddd13853157144aea8e6e0e6dca73cc6",
            "fbed7b516fce4cc8a4b67549554584e1",
            "f358fa9ba1dd4c2e8e87c27e169db120",
            "5768fa2ce08f4bf2bd4759e4b55238b1",
            "238ed8e09c124035832a0c41d6cecb9d",
            "ae2d45225a3d4c2284c7e15f46e47004",
            "220d22795c4a477eafd42919afb65b12",
            "fe6a45bb645643beb7e8350afa23f22c",
            "1bb710ffc39b4a3aa140e76a7b8d66ec",
            "1c84d2985e3a4c4caf954a958883b6ed"
          ]
        },
        "id": "8a2D4Cs3rbue",
        "outputId": "20bd10ca-e01a-49b2-db5f-d5b6fa790477"
      },
      "outputs": [],
      "source": [
        "# Now, we create an omnisafe Agent: We want to train the PPO algorithm, in the specified environment, with the specified configuration\n",
        "agent = omnisafe.Agent('PPO', ENV_ID, custom_cfgs=cfg)\n",
        "# The only thing we need to do to start training is call this function!\n",
        "agent.learn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdQsPDtorbuf"
      },
      "source": [
        "### Collapse/view training log for PPOLag agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "2c310a2ae17d4bafa70de8eb5e3d0860",
            "4ff1317a9a0b452cac7c656699d42cc3",
            "17b14611f4f543e3911e783610eccf26",
            "53657126aa814f95832d56b0083c433f",
            "cd409135810a4415a8b4d698251cbd52",
            "0a4b522cb4dc4663a1009e8f62fe97b1",
            "e9fb536764684882853d71756315579e",
            "a9baf29660f5482b8598d19755ab9307",
            "24d8b9280c4947959a3fb11f3704eec3",
            "bfa8f44d5f76454bb8d9828c668f9aa3",
            "e0a270785b62414ea4eadab823cb7bf5",
            "4a2178afde954fa49975cce74e0b32a7",
            "59ef6da7ac8541e9bd7be3621040cc86",
            "54a8c9685212462fb0294ec9ce9e20d1",
            "67626e3b09444d5491b9d1caeb703d11",
            "7109b6ec61ec419d92862a6f72f21689",
            "b2611c21d30f4f26b5630d7280a327fc",
            "dc6723cafe8e4066b57f42049a36ea9a",
            "b9414da230e74c1aad16a7b9f569d4a7",
            "fc714c6488e94f4c931fb0bc54fc03aa",
            "441fc94fcd0e4002b1ce0afd6b7ba733",
            "bc7acb5e1a3d46d89eb548f57539e58a",
            "ebda75774a744876bf2d7a0631784b25",
            "12f76c0d396b4aaca2551a12b6fa54ed",
            "f7b894e6b00f4c43b0dc0ff4651714a7",
            "be7530d2402d4d0183cd5418cad94cbd",
            "98c7ad673e7b46bf90d31ec4f64385c5",
            "45377030b7294f61ab9aa5024fae767b",
            "32199cbee1b64f519ed467a858c4770f",
            "b091cbb7c8474abfbe3931c04f2b9af1",
            "4498a42af40e4d85b80d505fea8d747b",
            "805a14d77a944066aa86304c2975e132",
            "90aa52643cf644e9a8d98df5dcee84d8",
            "0cb2296b9d804826b153c85a7ff336c0",
            "88eb48e378b542b2b93ebfb5c1b5eb25",
            "552c5b33ab2049d58a4442988b2d61fc",
            "d9323b6e971a4501a6991f5686228d5d",
            "280abe34edf2417cb693ec68926121fb",
            "b6ce6cfbf2d54195947c020f44a91a89",
            "892d06574b3441adb1765a3e1518efe3",
            "4c82064b0530411583e7e754515ddb44",
            "8c71a6773f644ab9a21e2a692ec9a963",
            "e874ac2fdca645e08db7b45794b1a4f9",
            "4a67007647c44a56b6d1cce3fdae435b",
            "b80019823b554513aa23ba430d3ad2ee",
            "80d4227b8a10457f84c3a5adbf513259",
            "c66ce98dd0dc4b328e6ee40c62df0b56",
            "6109f570e6fe4d4e9211777aad641388",
            "cfa4e15d90ea43758796903363526a71",
            "fec4c07587fe4515b1869b10890f73a4",
            "7032207c258c45bb9d3f42879c04e1df",
            "e8e1d15ad6eb40f9b27db79e6d06ba00",
            "0476d38eae154fa493b8288bdd6c10c6",
            "6d5bb21f4e434ce293354ceaefc5567e",
            "49997f1ec3544b30839b6c1c8ffec802",
            "f63be38239bb49798e9bc5c82c035c6e",
            "df5a5593969b40e4b5f9b8f743726b0e",
            "ec6cdc3b554c46549d94c13d398f50b8",
            "fe209b1137be48a48746236fc2fd43ad",
            "fe4632a44a2c4cf4ad1387876224b7b8",
            "e41b065cc5ad4b6cbc38599719126507",
            "91af4c050f804a5981bc7a99dd270ca9",
            "56e883dbb01347ecab0cd73540bb883d",
            "37b6a9ee5509421682879580ba5b9bc5",
            "fb5145af6d5a4385828af7d127e9c328",
            "75d1bac62be645899bb962a29e8cb639",
            "13fc6ce818e84595823dc3917611c8a4",
            "49b786b8313849a9bbcb487cf07eadc4",
            "a00d539fc8f641f182af415cb6e2932c",
            "28e7d2acad9645e195c81777054df341",
            "4820d18e18904baeae9d2cf6ee5f55cb",
            "df47c08dc3ae467b9361e11a4b9066c4",
            "d0fb5e26d3e64afeb06771783f1aac14",
            "62790692fdf8491e87af870110a7084e",
            "618ead23427f4dfe85f7dcc05155652c",
            "cf95cb53158641a8a4388ab4ab8a9fc3",
            "eb4f688cb298431bb5d52806e1ed1940",
            "ca51140057e649c4abd990a9db457934",
            "6bb83fa537e84155b87636cb0ed9d5de",
            "e27f31e0b2424db4892b39daaabf75db",
            "43ec6f5992fc4fca93f20b45e0493035",
            "fea73b3e313e4dab9f5a4af65856a653",
            "ef00d277bf9a404bbb4d2522687abcf6",
            "0ccb93e1b95749c6b630d41fd38dc231",
            "7f2f709975b1417eaf3c3615755af7e4",
            "eb34a7de78a1462cb4dd180bbcc3bbb0",
            "8644772e9762464888960d7c22e675c7",
            "922adbdf15c841148e16a155f36cbf73",
            "fae3e5a5e1304a9b8fece2baa058ada9",
            "c2238376b817411f8a3fc7a6a9c89c40",
            "93104632d56243398c74c32dbd02e52b",
            "9b7a88eb47aa4580a9e50b1528e3736a",
            "48f5751d802f47b38e1dbf1ae1e81117",
            "78ee82ff4de94af18807dbceb25c5e28",
            "582ca5dbb5d4497abb00da2130e0a427",
            "9f22df499aaa4d8ab8adfcefa94698c7",
            "250bb739f50042cbab42a837d084ed02",
            "4bedc35fa82b41b383e55f1376ee1cc7",
            "81e84d5b1a3d44dab110c8896cd2459f",
            "719703abbab94f808f9bcaf96b323638",
            "b327d6b19539460088b0e8b618418f2b",
            "c975b92f997541269a317a1738a4b39c",
            "00e87d0c625d4a64812505e9dac5e95a",
            "7c05248cf4b44456815b4c21d71f449c",
            "42aa0436beab4465a1b98c94eb2b2faf",
            "e1d6119fb6e141a89ce89e93b415f5e4",
            "ee1bae4a24eb4db2bff6aacdd9a0e2fc",
            "8e2097974620449dae5d1fd6398bc012",
            "b748a2ddd25a45f59756de38d21e057f",
            "c4692c15c1324026870e749a8c981d38",
            "4f6f013bcd124dd5869b5290bf973792",
            "deb4a282e7e148be88d4775be3ffb9fa",
            "be14196e138f453997fcfa9080025e95",
            "42864e89441b47779f5784a45c51d429",
            "90dbb40c12704f1887cec5ec2aed819f",
            "ae4fc34cf72e4dd2985ab97973a25b1a",
            "0680deccd0ee45cbb55cc22865e4713a",
            "946bf7cb1ee34a36a530f7cb05b0ef30",
            "311b5ae9ac10496abd7bf5b03fbd2357",
            "8d017bcf6aea4d688711c1f2436942fd",
            "fde0f7242b3f4561a4f92e32ab1ed4cc",
            "626b3b28a57d480abdaa0c386767faa8",
            "935514ee34364c1ca12423570d83bf1c",
            "e143df7ea67643ca817be0822b746a56",
            "d832a5a199044ba88a49d6145df71e55",
            "54ef82f9f83643bb863ae0c17f17999e",
            "c3069c86fcfa4fedaf61b842950e2735",
            "662f73a9baee427aa5b10bdd5dc8265a",
            "4579cb0a7efc4dec86c4f6fcaaa671e9",
            "1090c0f966d84eb2aa09c8bc033175f7",
            "73ad8df9dec14791a60683f45ff1ae15",
            "9c0191d44ae049e98802f90b358c1aec",
            "310fc2dc58f44cd58221d770d44b99fa",
            "8eac1e0d444745118d9f7e4a8c24a551",
            "d61fc4c1c2814b0ba2b07d847a3ec6f1",
            "0933fd191f99481fb2a384c8ab61cea3",
            "fe58b5ce93444d6b8407e148c0f5a77c",
            "b426971139b24c1f8377c21fe8bed272",
            "5a8128ccc99847fb8dba0ecb70cbb7e6",
            "0e2fe0da155148a092c7a96a94d0911c",
            "59e522460b61444eb40f23f8f2b1714e",
            "d49ff4db906d43a9a6dbb858689d0c84",
            "7a2cfe9702c048eb959c38acdd540001",
            "bf38722f39f04a438e987166c95262a9",
            "9f7a2b0f11f34aa4abc1b2188a3ea65e",
            "c38094fed3c94a52933eacf26f2d65c3",
            "c4ba992202cd44508e8b07a3805da51d",
            "c8756b317c714bab9736213c53b2bb51",
            "0a73ec917dec45d58a4d9c2c1537a9c6",
            "f6fd67110af64117bcf67c168b07aa98",
            "8faac0887758449d830ed771e1e34b07",
            "7119750464df44bab853e0d9b92d6f68",
            "b173abd8a57649ba9d417ad888fe02d1",
            "7297a44342854f2ba4922023be5daedc",
            "2ccd3dd46e254557a612f45ae38ab2cd",
            "7632d2e0de6945be8a992404abd58bf5",
            "7dd1e5a9b17e49439454bbb6441ad559",
            "0f2ecb4e85ec434097af25d55507b903",
            "d9f4125548f746049789682116d64f67",
            "3d68cd88d5e949009ee0feefbe5bea6b",
            "7d70a1b095df48c9b8a05e4feeed1001",
            "01c0b949c2214a25b881a34412d18357",
            "11eef729eae44a90b59960e0116bfaee",
            "0b768f26e3e64a9cba509ba5205922e2",
            "f536f875a2b245e6a81e124958a89c30",
            "24dcf61ceaff40b99758033c05285ef0",
            "8aaab8d726c94ae787600e6fd774d58a",
            "74328de1d3f04cb3a9e85adf6564412c",
            "daa28a0ace254868a0cb1c85917890c7",
            "80f355a3f69c4447a4dbdbeb58c6a858",
            "66ccde1ff8f9458790f4642867c48bac",
            "313d9e9f2854467aa7c62fa54cc97cbc",
            "bfaa26921021428ba55f6dfba5439ea0",
            "231c7c94a2d0454b96b5cdae7b6e2884",
            "6bebf0482e004096a725344d5e74133c",
            "c05a7d6cbfbc4bbf8ea78da2ded86a4a",
            "17136d59ec0849c28b3b45a674e37cb3",
            "5e8fa36bdfed4ce895c2c5e0190c630e",
            "dcb167e7d80b45eab2f024e10ab367bc",
            "6e9a952f772b4df5aa92b745f0a26c3e",
            "862cbe50b3a94832b10d73e79e43d84f",
            "419f663191484a23a3ec50f91b93ef5d",
            "4399ab8a15434b3bbae85e2987928d00",
            "f3e045caef674063a35de681a2836f5c",
            "9cd1b1d17bf14c9584a2e602bd8c0545",
            "fd993aa0af844dc39f98575723bf5b2c",
            "fc6a2c7c066847628cb1b56bcb7e3b8b",
            "ac405d00716a44bf99c3c3a908881be4",
            "ec65d007f14b4ba68390cb34b413b1d9",
            "e58170c401184632aea1cf4a34ff5420",
            "5b4f6f964c4f4c628c4ab32fdf3f4ad1",
            "b7bb60d2fca2491fae636d067f9e6e21",
            "0496a9eb70b94affabf9c05d9dec3a89",
            "bec16f96b4594b888aff051d100804bc",
            "43172251d90249d98b7ccbab5c324cf6",
            "6b53c5e0861a4e0eb45f7c0a32494df1",
            "3590f178a06a4878983a0264de874b7e",
            "78a94129688e4be6896e3e068f6f7823",
            "3081c130a07541278408b8572ee381a4",
            "ffb8b9f3cfa94416b050e86e9c196b58",
            "21fc78a4af154135825e896b4a378d4b",
            "2867b11e0f514ea3bd9b4721e2606efe",
            "2773db4edd184e3d887fc5b89e87cf4b",
            "532d078fca1145d291f9b57fbab753f0",
            "ca59d7d01aa34aa28559246956dcf8d7",
            "cdc0d1c2e3f94c0a854c88cba2507c17",
            "42b1559201094d1ea3116e19b6926a78",
            "59bed232383e4f51ac6ac3a2172e9cf9",
            "50d0902a9e4b4f858e60bf599209136f",
            "b84ec9f76a034461bfd3b7019d50807a",
            "55c78d558d1a4511851178ee3241c69b",
            "125314cb1d2446d88e2aea598328f6f3",
            "9fed2592c90e4381b0ac0ea93a7751ac",
            "fd11e003a02c4ca0ac4579ef111d7895",
            "b4c10a72379a413eac5a315d9f549e11",
            "02970e41c921430fb15bbe552a1b5c9c",
            "bd0d49c01aa44aefbd3a1adfed5348ac",
            "a1d7e9fbc3e440ac94763cd7bd69733c",
            "03d3d9543bae49b09450d77ef44fc436",
            "ce235062981a4c2c8b42dac32ce3b93d",
            "952345644a7c4aa0b31c178ca19b2e9b",
            "3d46eb56ae45493fa81ab47000deea06",
            "af47c25e505c4403960105ec3257c382",
            "632dd2fed3484843b3f92bb968e34f5e",
            "9f06857dfa9d4f8e8a4a48329300b644",
            "63973bfe6fe649b8935dce9ff1d29990",
            "fffe85efd37347c4a9cef881e5db8fae",
            "f7c86f0a1d0c4813a1c504c639ec3b89",
            "95a89229a1e14d8da0579b20c939e921",
            "510d98b439ac4c4a8c4e1fe5f92a1f77",
            "ac6d97ac61654d3597789112e164230c",
            "b434178b200e4fd3a2ebb763940df30a",
            "6315cfceb1f94d29bf0b1d752527e9dc",
            "5ad661a8fcdd4c658cd03547ce24443f",
            "786d5902d8c44b918bec36b2a7af78ae",
            "99400ab5f87b41968fa8b37ff450fb9d",
            "70ad2672b2274bdfad700431af5a4a66",
            "582932f41121432a8f00112f3cff115f",
            "dff617bda06f40be9d23374f2f852081",
            "398eb64353994c0fb3a43b725d8ce83c",
            "f55fc22d450a4a2ba598b86e19f9ce48",
            "d11fb0c40c5546f29052c2afa460dc9b",
            "e540df064d6446be9bcff82b82a8677f",
            "0cd1aedad9e44071a2e1fc822d0a320c",
            "8033eaef6d9b456fa148ff986022fe88",
            "85f9bd3324594f258b2da2fce3e2a8a7",
            "472eaf4246224ff9a0c7296b6b9b94df",
            "147bf15e5850496faf6ce2b43249b069",
            "4e65c60601824494a6ec131c11859ed4",
            "fdb3932d14e240d7b5773a0362bd15d5",
            "9173f980ffc74c02a124380c74271d3e",
            "3ac42fac02e446598325a439261ec9a8",
            "860290047e85443cb2751ac2e29b83b9",
            "c69a888df28d4f979cc7c3068785a88e",
            "3dace0f756f74e87b8ed9ff5b41c2649",
            "c75fb2daece144639d383a2c7033649a",
            "3ee0ccf4d2e24ad0933b49cd7c3c6f3e",
            "438215b8440040d9b49ce2a8a0411339",
            "e9444e9be3164f3ab46805ee186e0002",
            "c6c7ccd8593345a2b69b637b32cb4936",
            "b358dff239ec4afda49478962f9e62c7",
            "0e69a1142c284e9b8d4ca8ee96d85c6a",
            "54d91a738dea459e9811b02257aecf87",
            "69058810f8f94ea691ca75f1eb335c27",
            "6a03999748d44f27aa15d057801a2672",
            "408ead10709f43c1a01b74185be00cb8",
            "3ed610b37662432a9a6aeb1557f123fa",
            "6002c3c1e16c42cca4d8a1f45225da85",
            "2455f03eb9f14d18983d93ccf64d4a5c",
            "6885860e56cd4f21afdb3f5596eb085e",
            "ecbaa3231a1e48c2a5a0fa1fa6d133b9",
            "1f17d434844746efb738db89532ca33e",
            "e0d559b850944aad81df5d0d66410814",
            "e7df152983f64d989792943c34e59925",
            "3e6e9217b4b949ddb9a7cbcb5c659ddb",
            "362cba2438314ebd9a18c2278b5fa832",
            "5e85fe3ab84242d9944dd72f318873e1",
            "857b9b39983f4299a774e4dad8ee36d4",
            "1e5c61ef3bce442882ffad4ac4f13ec2",
            "e594153913d747d29431bc332c583673",
            "ed369d497b244357a39d7354cc17f793",
            "62780ef2b16644219fb6e8f1388c67f6",
            "6d12a3800cbd4d23b6e1c4ad60b62b94",
            "25300df34a394cd191edf40a2b72b8b5",
            "fd400e7657114650990c4016057455ca",
            "87b682eb44f64edca626e956b05655ae",
            "317322e80a11461eba870b4d107d73f0",
            "4515f948df1f46d996e03e8789ba5fdb",
            "f87251c032ff470884f2bc9679ebd22d",
            "a6872fcb444349c08d6d8ac1e990f118",
            "feb4d7f0fe9f4ca2bc756b9402cde728",
            "617b79f5d85d4bf6b76cccc444a27418",
            "a25f23f62a4e4e91a95fb0957e7b6cc0",
            "587b531701d3441f8d440d6b4e3f85ac",
            "c962a57257b9451eb7bafbe64959433e",
            "fe41bd0c14da46bf98222877fd697fd6",
            "b6f4ac4c9f38447ca69b56a47990ad57",
            "66f6483f4a2248859bfd4ed05ee49c33",
            "0a61dc0f48b84064a2b3cf6bc2fe6e0a",
            "b4353aa9fbcc43669e698b90d3e7e2e8",
            "30d32b0ca97c43118d5d44ded4a6d16a",
            "c557907e0fe8488588b65ec7ac483f06",
            "d23756de322a44dcb8d695d14a84b198",
            "28d9fb5df5734a67b94a204b7e0cb266",
            "8b6219d68a3d42b1a1ae60bc9f06f5da",
            "0d5ba3c9642447e399ed181f0e565c85",
            "72c505b1a5fc4725bbdfacfd129320d9",
            "00daa07e15204ea5b87aa6bf4733ad9c",
            "9497dc2ff1194bdbaa871f2986e4ad29",
            "2224c5b13eff47ab8e0b9b40887c190e",
            "5c47528d430e40279ee43c30f4b97ca3",
            "054c1e262c9d4282b9057a10a264c35d",
            "243cd8d9503f4e33badbfcb307734876",
            "95ca93706a2b4079a98ffc5f640f9a4d",
            "3b46a9ac064d4e628cf3b4f4543d4969",
            "f2456989003a4802b57ef70aeb904294",
            "c75a1ac1adfa41828d0045a13e2a34f2",
            "fbdaef0fff2e47fb9321dcc1e5f10e1e",
            "befc954c42a64a11887fa27ae4c21579",
            "ccb10f2f23684e1497d9c9550ba52814",
            "0bf208696052461ca4c5dd40b8eb2f65",
            "988378d0a91943bfb86f0bcc515fbba5",
            "97668c7ec3a840fc965ef956dbc90eaf",
            "5106fa5fae4d48cba02cf2bb9a80ec79",
            "a382bc4c25f749a1a5917cb67e2cdb53",
            "8924826c3c8f4bb9bd69b14430b24c2f",
            "a8e11b315fe843b68f07c34c93445787",
            "1a3c0c06e3574b7dbfca0013fb36dc9a",
            "8241228ff206440da290fb7feb93804c",
            "bc1208ac586d49dbb07885ecbbb297ee",
            "d58f37d84d9f46708a2216ce9e4addb1",
            "b67e3b30eb52478996220dfccc2a494d",
            "c927c38dcc8f405ea33c3aa91eb91d4e",
            "e9ca8fd2760248aead5ffcb4460d97c8",
            "ae01e74c59504855b71e920b954a19b8",
            "b2a13c6cfe8f4cdf94ebcfda0b13600f",
            "b584516fb1d048deb69f24873693c988",
            "80967c3ab6d844da9d5b71fdb080a13a",
            "99a7e070e5034d209aab1d20f2cac9b4",
            "1dbde786fd6f40f4aa35ce118a355908",
            "61b68931443e4a5c8377728f9830bc08",
            "0303d971657340d1a80a2a028fb4fbd4",
            "490212ee4a9e4ffbb4e543d8ed3ec24e",
            "68e73a9014f04117985bc26d37fd4f08",
            "8a998ffec42a4928bba639ce23b0765d",
            "757ad5a6e7b44cf7b74cfa18f931e2d5",
            "f971fa2172034e219100e7a587d4c3aa",
            "71dc495717274f92a81222db7b89ada5",
            "d29bc3f39b7f4939a7adfcf81b5f64f6",
            "a156934200c94c4dbc2e8f60b35a1fd2",
            "29fea529142c404e9b215c2ff38a9cf1",
            "f5cfc6029d0449b3b591a344d5ed9bca",
            "c923ab53585343da8690f020554a8a59",
            "3df2f366cd344124aa8ce4f3c8549a77",
            "f1b395aebad44b75953c405edf099b63",
            "fe8633eda3b044299f78f8172bb18dd0",
            "11c408218fc047c181f472e4378f4393",
            "5c6ace424d9e4492b3084fa4af3c695a",
            "acbd1e2793c74811baee58882fe09514",
            "4feef594963d4cc7b1772aa84abe3cb4",
            "ce30e272e4a24fb6addad123f88bb252",
            "5c4ba39e0c074133975fe2f94a00ee18",
            "b065aba27ebc48a9b71de768b4c81fbd",
            "40c885649d7c4534b9d90cbfaaf713d3",
            "739b407a17f4406998538f8238960419",
            "2b2371adfbff4284ab38206d6b9faac8",
            "9e4ba51b2b8b42b4b77d766585de087f",
            "c1a6933d4249492fbd529efeced42195",
            "231d686a07c64fcbbec2b54adb4d57f3",
            "2ce0e7eea7d649f2850862fab84983e0",
            "50f00f26c68246e39f5c46ba57e970a3",
            "e8600d379c1640fcb08b38ac34ad2fdb",
            "252deb2f74f543e0b1ae5aba11ce0d41",
            "2cc588a37bd643e9a08a0a0958ddef9a",
            "ff5a2d0fd85a49788cd0cbd004f03407",
            "02a4e787c732410fa557b85dbd971b6e",
            "5e560d668f084b77a0d0c17ac28c2b11",
            "596a20cbc4c24834950ee0d98039374d",
            "e5c7c8b9bf134a76a500e55392f799d7",
            "705459ee52544dd3a810f827a2afee27",
            "ea19b6419f2c49c5ad566c676b735205",
            "edb0d948655f4b26b30e8a2d79440ba1",
            "a1323cdc4ecd49b3a2289020cfb461ce",
            "feeff0ade7e5481581f0dcf461169775",
            "f74e3e20abc544618e2598f57f2cab02",
            "3a3098efc4e049e784f92739b04832e3",
            "9d7357b99827499aa71509342172bd44",
            "729cd829aeb047a0b5e925154406d248",
            "94ca26260cb74d0e8f83445443f24c65",
            "4588217fec6a47be9ef0ce52dd5724f0",
            "7e5e9fedbc784ecc9eafb71f99d38a80",
            "c99b43f46df6419aab300c9e4b0b4404",
            "2a566f4d7cc240e19d6e4dd7027ed72a",
            "cb4f2150f8c64daabb94493b00163469",
            "f880f6c7f8a34e77a99f175f276f7503",
            "f3b1fd9b36c349d79d6baa2fd1b896eb",
            "6efb0f9881794d378382fd40ecef1376",
            "4923f4cc68bb4068a0262a7a459e595b",
            "4c889403721a46a8924895af46e74cb3",
            "f86e04ce8e3546bf90630f2bc09f3172"
          ]
        },
        "id": "xSkOl5TCrbuf",
        "outputId": "f139a43a-7034-4c84-aea8-491353ed4afc"
      },
      "outputs": [],
      "source": [
        "# Now you get to do the same thing, but for Safe RL training!\n",
        "# TODO: Create a safe RL agent that trains the PPOLag algorithm in the specified environment, with the same configuration data\n",
        "# Then, run the training!\n",
        "\"\"\" START CODE \"\"\"\n",
        "\n",
        "\n",
        "\"\"\" END CODE \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8Gx238Rrbuf"
      },
      "source": [
        "### omnisafe evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxVm2qIerbuf",
        "outputId": "a4529d3f-ba20-4283-90e8-5a33f53645e0"
      },
      "outputs": [],
      "source": [
        "# Evaluation in omnisafe is also rather simple!\n",
        "# agent.plot() will plot the development of rewards and costs over the training steps\n",
        "agent.plot()\n",
        "# agent.evaluate() will evaluate the agent, and output the average cost and reward.\n",
        "agent.evaluate(num_episodes=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TfDeu0orbug",
        "outputId": "6f9c0bad-6178-4733-96dd-202e714442d0"
      },
      "outputs": [],
      "source": [
        "# Now it's your turn again!\n",
        "# TODO: Plot and evaluate the safe agent.\n",
        "\"\"\" YOUR CODE \"\"\"\n",
        "\n",
        "\"\"\" END CODE\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imVzzX1nrbug",
        "outputId": "27113181-dd01-4eff-f69f-60a3410f7b31"
      },
      "outputs": [],
      "source": [
        "# We can also render an episode of the trained agent by calling agent.render().\n",
        "agent.render(num_episodes=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-GJTUklrbug",
        "outputId": "45c480d5-8c33-4b50-adf4-c240fd28deb5"
      },
      "outputs": [],
      "source": [
        "# TODO: Now, render an episode for the safe agent!\n",
        "\"\"\" START CODE \"\"\"\n",
        "\n",
        "\"\"\" END CODE \"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lab_env2",
      "language": "python",
      "name": "lab_env2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
